---
layout:     post
title:      深度学习基础-神经网络权重初始化
subtitle:   深度学习基础-神经网络权重初始化
date:       2021-09-29
author:     JMX
header-img: img/post_bd_04.jpg
catalog: true
tags:
    - 深度学习
    - 权重初始化
---

# 一、两个问题

> 假设3层神经网络,输入节点v0，第一层节点v1,v2,v3 第二层节点v4,v5 第三层节点v6。其中vi=f(ai),i=4,5,6 f为激活函数。

**前向传播：**

![在这里插入图片描述](https://img-blog.csdnimg.cn/83cc1293388e439ab8a1d33e6c67396f.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBASk1YR09ETFo=,size_20,color_FFFFFF,t_70,g_se,x_16)

## 1. 全零初始化是否可以

一般情况不可以。当全零参数初始化时，除输入节点所有节点值均为0,根据上式除第一层梯度与输入值有关其余均为0.

LR等一层网络可以全零初始化， 网络梯度与输入值有关。仅全零初始化一层也不影响训练，但涉及两层及以上，从涉及层到输入层的梯度都为0,参数无法更新。


## 2. 参数全部相同初始化是否可以

不可以。若初始化为相同的参数，隐藏层所有节点输出相同，梯度也是相同的。相当于输入经过一个节点。

# 二、参数初始化方式

## 1. 预训练初始化

**pretraining + finetuning**加载已训练好的模型参数，进行下游任务的模型训练。

## 2. 随机初始化

### 2.1 random initialization

`random initialization: np.random.randn(m,n)`

随机产生符合正态分布的m×n维向量

弊端：随机会产生梯度消失，随着网络层次加深，由于链式求导法则，输出越来越接近0

### 2.2 Xavier initialization

`tf.Variable(np.random.randn(node_in,node_out))/np.sqrt(node_in)`

![image](https://pic3.zhimg.com/80/v2-6302a7093b93e1376e54e95033c58086_720w.jpg)

M 代表着输入输出维度即node_in,node_out

保证输入输出方差一致

### 2.3 He initialization

`tf.Variable(np.random.randn(node_in,node_out))/np.sqrt(node_in/2)`

适用于RELU激活函数，只有半区有效

## 3. 固定初始化

比如对于偏置（bias）通常用0初始化，LSTM遗忘门偏置通常为1或2，使时序上的梯度变大，对于ReLU神经元，偏置设为0.01，使得训练初期更容易激活。

# 参考链接
[https://www.leiphone.com/category/ai/3qMp45aQtbxTdzmK.html](https://note.youdao.com/)
